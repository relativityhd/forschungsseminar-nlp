{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia dewiki\n"
     ]
    }
   ],
   "source": [
    "import mwxml\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "dump = mwxml.Dump.from_file(open(\"../data/dewiki-20220520-pages-articles-multistream1.xml-p1p297012\"))\n",
    "print(dump.site_info.name, dump.site_info.dbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155401it [00:40, 3855.27it/s]\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*')\n",
    "list_of_words = ['hersteller', 'unternehmen', 'Unternehmen', 'Hersteller']\n",
    "words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "#lines = []\n",
    "with open(\"../data/title-categories-map.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump)):\n",
    "        line = \"\"\n",
    "        line += page.title + \",\"\n",
    "        for revision in page:\n",
    "            for match in pattern.finditer(revision.text):\n",
    "                line += match.group(1) + \",\"\n",
    "        line = line.strip(\",\")\n",
    "        #lines.append(line)\n",
    "        if words_re.search(str(line)):\n",
    "            outfile.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1579\n"
     ]
    }
   ],
   "source": [
    "num_lines = sum(1 for line in open('../data/title-categories-map.txt'))\n",
    "print(num_lines)\n",
    "#Output 1579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 2c) \n",
    "\n",
    "1. Infobox der Unternehmen extrahieren\n",
    "\n",
    "Der Aufbau einer Infobox eines Unternehmens in dessen Wikipedia Artikel ist streng definiert. Die Infobox beginnt mit zwei geschweiften Klammern und dem Typ des Eintrags, in diesem Fall {{{Infobox Unternehmen. Sie enthält maximal 14 Einträge: Name, Logo, Unternehmensform, ISIN, Gründungsdatum, Auflösungsdatum, Sitz, Leitung, Mitarbeiterzahl, Umsatz, Stand, Branche, Website. Erforderlich sind hiervon Unternehmensform sowie Sitz. \n",
    "\n",
    "Die Infobox wird mit folgendem Regex Pattern definiert: ((?<={{Infobox Unternehmen.).*?(?<=Homepage).*?(}})).\n",
    "Es wird der Inhalt nach {{Infobox Unternehmen bis einschließlich dem letzten Eintrag Homepage.*?}} ausgelesen. \n",
    "\n",
    "Verbesserungsvorschlag: Einträge einzeln auslesen, sodass nur die Parameter ausgegeben werden, die einen Wert besitzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155401it [02:41, 959.30it/s] \n"
     ]
    }
   ],
   "source": [
    "#getting the infobox of a company\n",
    "\n",
    "infobox = re.compile('((?<={{Infobox Unternehmen.).*?(?<=Homepage).*?(}}))') #regex pattern gets text between keyword 'Infobox Unternehmen' and 'Homepage' \n",
    "\n",
    "with open(\"../data/infobox.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump)):\n",
    "        line = ''\n",
    "        for revision in page: \n",
    "            revision = re.sub(r'\\n', '', revision.text) #removing linebreaks \n",
    "            for match in infobox.finditer(revision):\n",
    "                line += match.group(1) + \"\\n\" \n",
    "                line = \" \".join(line.split()) #if there are multiple whitespaces, all except for one get deleted \n",
    "                outfile.write(line+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697\n"
     ]
    }
   ],
   "source": [
    "num_lines = sum(1 for line in open('../data/infobox.txt'))\n",
    "print(num_lines)\n",
    "#Output:697"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun soll auch für Unternehmen ohne Infobox deren Namen sowie dessen Webseite ausgegeben werden. \n",
    "\n",
    "Hierfür wird im ersten Schritt geprüft, ob der Wikipedia Artikel des Unternehmens das Wort Infobox enthält. Dies wird mit folgendem Regex Pattern geprüft: (?!.*?Infobox)^.*$ \n",
    "Enthält der Artikel das Wort nicht, so erkennt das Regex Pattern den gesamten Artikel. \n",
    "Wird der Artikel als Unternehmensartikel klassifiziert und enthält er das Wort Infobox nicht, wird der Artikelname (=Unternehmensname) ausgegeben. \n",
    "\n",
    "Um nun auch die Webseite des Unternehmens ausgeben zu lassen, wird folgendes Regex Pattern angewandt: \n",
    ".*((?<=Weblinks...).*?(?=...Einzelnachweise)).*\n",
    "Dies gibt alle in der Sektion Weblinks eingetragnenen Webseiten aus, worunter oftmals auch die offizielle Unternehmenswebseite fällt. \n",
    "\n",
    "Verbesserungsvorschlag: Es werden viele Artikel ausgelesen, die die Kategorie Unternehmsart enthalten, aber keine Unternehmen sind. Diese sind oftmals allgemeine Informationsseiten und enthalten daher auch keine Infobox, weshalb sie vermehrt ausgelesen werden. \n",
    "Darüber hinaus wird für die Unternehmenswebseite die ganze Sektion der Weblinks ausgelesen, die oftmals auch andere weiterführende Links enthält. Eine Verbesserungsmöglichkeit bestände darin, den Namen des Unternehmens in den Links zu suchen und nur die Links, die den Namen enthalten, auszugegeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155401it [00:48, 3185.01it/s]\n"
     ]
    }
   ],
   "source": [
    "#Companies without infobox\n",
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*')\n",
    "list_of_words = ['hersteller', 'unternehmen', 'Unternehmen', 'Hersteller']\n",
    "words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "no_infobox = re.compile('(?!.*?Infobox)^.*$') #gets the whole article if it doesn´t contain 'Infobox'\n",
    "pattern_website = re.compile('.*((?<=Weblinks...).*?(?=...Einzelnachweise)).*') \n",
    "\n",
    "\n",
    "with open(\"../data/company_name&website.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump)):\n",
    "        line = \"\"\n",
    "        line += page.title\n",
    "        website = \"\"\n",
    "        for revision in page:\n",
    "            revision = re.sub(r'\\n', '', revision.text) #removing the line breaks \n",
    "            for match in pattern.finditer(revision):\n",
    "                line += match.group(1)  \n",
    "        \n",
    "        if words_re.search(str(line)):\n",
    "            for match in no_infobox.finditer(revision): #getting companies without an infobox \n",
    "                for match in pattern_website.finditer(revision): #getting the weblinks which contain the company url \n",
    "                    outfile.write(page.title+\"\\n\") #printing the page title = company name \n",
    "                    website += match.group(1)\n",
    "                    outfile.write(website+\"\\n\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_without_infobox = sum((1/3) for line in open('../data/company_name&website.txt'))\n",
    "print(companies_without_infobox)\n",
    "#Output: 136 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um nun eine Kurzbeschreibung des Unternehmens zu erhalten, wird der erste Satz des Artikels erfasst. Dieser enthält bei Unternehmen den Unternehmensnamen eingefasst in ''' '''. Aus diesem Grund wird folgendes Regex Pattern angewandt, um den ersten Satz auszulesen: \n",
    "((?<=\\''' ).*?(?<=\\.)+) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154157it [00:41, 3682.78it/s]\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*')\n",
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*')\n",
    "list_of_words = ['hersteller', 'unternehmen', 'Unternehmen', 'Hersteller']\n",
    "words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "pattern_firstsentence = re.compile('((?<=\\''' ).*?(?<=\\.))')\n",
    "#Erster Satz fängt meistens mit dem Firmennamen in ''' ''' an. Dann wird der erste Punkt gesucht.\n",
    "\n",
    "with open(\"../data/firstsentence.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump)):\n",
    "        line = \"\"\n",
    "        line += page.title\n",
    "        firstsentence = \"\"\n",
    "        for revision in page:\n",
    "            for match in pattern.finditer(revision.text):\n",
    "                line += match.group(1)  \n",
    "        \n",
    "        if words_re.search(str(line)):\n",
    "            for match in pattern_firstsentence.finditer(revision.text):\n",
    "                firstsentence += match.group(1)\n",
    "                firstsentence.replace(r'\\*', '')\n",
    "            outfile.write(page.title+\"\\n\")\n",
    "            outfile.write(firstsentence+\"\\n\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstsentences = sum((1/3) for line in open('../data/firstsentence.txt'))\n",
    "print(firstsentences)\n",
    "#Output: 1570"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('forschungsseminarenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe6018fde3cbbb4eddeb8a40ce7dd82a54a8d34f7d7f0c961465b9161df6ac74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
