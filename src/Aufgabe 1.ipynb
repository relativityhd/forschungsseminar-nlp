{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Download der Wikipedia Daten\n",
    "\n",
    "Das herunterladen der Dump-Datei kann über einen der folgenden Befehle ausgeführt werden:\n",
    "\n",
    "```sh\n",
    "# Please run the commands from the root directory of this repository. To check run\n",
    "$ pwd\n",
    "/path/to/repositories/forschungsseminar-nlp\n",
    "# Download subset for testing/ development\n",
    "$ sh ./scripts/download-dump.sh\n",
    "# Dowload everything for application\n",
    "$ sh ./scripts/download-large-dump.sh\n",
    "```\n",
    "\n",
    ">Diese Befehle **müssen** im Root-Directory dieses Repositories ausgeführt werden!\n",
    "\n",
    "Zur Auswahl stehen hier\n",
    "\n",
    "1. eine Dumpdatei mit einer kleineren Größe von ca. 5GB (im Prozess entstehend) für ein effizientes Arbeiten und\n",
    "2. die allumfassende Dumpdatei mit einer Größe von ca. 28GB (im Prozess entstehend)\n",
    "\n",
    "Über die Variable `ONLY_SAMPLE` kann bestimmt werden ob nur ein Subset der Daten beispielhaft verarbeitet werden oder der komplette große Dump verarbeitet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLY_SAMPLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraktion des Dumps:\n",
    "\n",
    "Die Bibliothek \"MediaWiki XML Processing\" oder auch \"mwxml\" ermöglicht ein vereinfachtes Streamen XML-Dumps über eine Abstrahierung.\n",
    "Über den mwxml.Dump werden die Wikipediaartikel auf eine iterative Weise über mwxml.Page‘s zugänglich gemacht.\n",
    "Im Folgenden werden über die angegebene Datei (entweder die kleine oder große XML-Datei) ein mwxml-Dump erstellt, welcher eben diese Attribute bereitstellt.\n",
    "Um die Funktionalität und Fehlerfreiheit zu prüfen werden im Folgenden zudem die Info der Seite und der Name der untersuchten Datenbank ausgegeben. Zu erwarten wären hier Hinweise auf die deutsche Wikipediafassung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia dewiki 155401\n"
     ]
    }
   ],
   "source": [
    "# Importieren der Bibliotheken und Module\n",
    "import mwxml # Für Untersuchung der XMLs\n",
    "import re # Für spätere regex-Operationen\n",
    "from tqdm import tqdm # Für die Integration von Fortschrittsbannern, Ladebalken etc.\n",
    "\n",
    "if ONLY_SAMPLE:\n",
    "    dump = mwxml.Dump.from_file(open(\"../data/dewiki-20220520-pages-articles-multistream1.xml\"))\n",
    "#dewiki-20220620-pages-articles-multistream.xml als Alternative, falls großer Dump genutzt werden soll.\n",
    "    dump_len = sum(1 for _ in dump) # Das verbraucht den 'items' generator des dumps, weshalb 'dump' neu geladen werden muss\n",
    "    dump = mwxml.Dump.from_file(open(\"../data/dewiki-20220520-pages-articles-multistream1.xml\"))\n",
    "else:\n",
    "    dump = mwxml.Dump.from_file(open(\"../data/dewiki-20220520-pages-articles-multistream.xml\"))\n",
    "    dump_len = sum(1 for _ in dump)\n",
    "    dump = mwxml.Dump.from_file(open(\"../data/dewiki-20220520-pages-articles-multistream.xml\"))\n",
    "print(dump.site_info.name, dump.site_info.dbname, dump_len) # Ausgabe der Informationen zum Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Sammeln der Firmennamen aus Dump\n",
    "\n",
    "Im Folgenden wird vorläufig eine später verwendete Methodik und Funktion zur weiteren Verbesserung der Firmenklassifikation implementiert.\n",
    "Für eine spätetere Ausräumung von Unsicherheiten in deutscher Wiki werden sowohl deutsches NER-Modell auf deutscher Wiki und englisches Modell auf englischer Wiki genutzt. \n",
    "Auch mit deutschem Modell auf rein deutscher Wikipedia möglich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.23.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (60.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.3.0/de_core_news_sm-3.3.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from de-core-news-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: jinja2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.7.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (60.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm # Herunterladen des englischen Spacy-Sprachmodells per Bash-Skript - erfahrungsgemäß bessere Zuverlässigkeit.\n",
    "!python -m spacy download de_core_news_sm # Herunterladen des deutschsprachigen Spacy-Sprachmodells per Bash-Skript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die nötigen Modelle heruntergeladen wurden, kann nun die Funktion zur Verbesserung der Genauigkeit implementiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firmenerkennung über NER\n",
    "import spacy # spacy ist eine Bibliothek für fortgeschrittene Natürliche Sprachverarbeitung\n",
    "import wikipediaapi # Vorteil: sowohl für 'de' als auch 'en' nutzbar: die wikipediapi ist eine Implementierung der Wikipedia-Api als eine \"Wikipedia Python API\". So können sehr effizient Zusammenfassungen und Seiten zu gesuchten Titeln angefordert werden.\n",
    "\n",
    "\n",
    "en_nlp = spacy.load(\"en_core_web_sm\") # Laden des englischen Spacy-Modells für NER\n",
    "de_nlp = spacy.load(\"de_core_news_sm\") # Laden des deutschen Spacy-Modells für NER\n",
    "en_wiki = wikipediaapi.Wikipedia('en') # Laden der englischen wikipediaapi-Version\n",
    "de_wiki = wikipediaapi.Wikipedia('de') # Laden des deutschen wikipediaapi-Version\n",
    "\n",
    "\n",
    "def reduceUncertainty(page_title): # Diese Funktion soll abgerufen werden können, falls bei einer erkannten Entität nicht klar ist, ob es sich um eine Firma handelt. Hierdurch sollen Unsicherheiten reduziert werden.\n",
    "    en_page = en_wiki.page(page_title) # Das Funktionsattribut page_title enthält den gesuchten Firmenname/Wikipediatitel und wird hier in der englischen API gesucht. \n",
    "    de_page = de_wiki.page(page_title) # Das Funktionsattribut page_title enthält den gesuchten Firmenname/Wikipediatitel und wird hier über die deutsche API gesucht. \n",
    "    org_list = [] # Diese Liste wird leer erklärt, um später alle erwähnten Organisationen in dem Betrefflichen Firmenartikel zu erkennen.\n",
    "\n",
    "    if not en_page.exists() and de_page.exists(): # Englische und Deutsche Wikipedia-Seite existiert nicht\n",
    "        return True # gibt True(\"company exists\") zurück, wenn die Seite nicht gefunden werden kann. So werden nur Seiten entfernt über die wir Zusatzwissen erlangen konnten und am wenigsten drastisch eingegriffen. Je nach Business Case kann dies anders Implementiert werden.\n",
    "\n",
    "    # Deutsche Seite existiert aber Englische Seite existiert nicht - in diesem Fall muss idealerweise ein deutsches NER-Modell verwendet werden\n",
    "    doc =  en_nlp(en_page.summary) if en_page.exists() else de_nlp(de_page.summary) # Die Zusammenfassung sollte in den meisten Fällen für die Urteilsfällung ausreichen\n",
    "    for ent in doc.ents: # Alle erkannten Entitäten werden hier gesammelt\n",
    "        if (ent.label_ == 'ORG') and (page_title in ent.text): # ... Und anschließend gefiltert: Zunächst sollen nur erkannte Organisationen behalten werden, welche zudem in ihrem Eigennamen den Titel der untersuchten Seite als String enthalten. Zukünftig könnten hier erweiterte Regex-Operationen verwendet werden.\n",
    "            org_list.append(ent) # Nur diese, den Kriterien unterliegende Entitäten, werden der Liste von erkannten Organisationen hinzugefügt.\n",
    "    return len(org_list) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Obiges Skript durchläuft also den folgenden logischen Prozess bzw. Algorithums:\n",
    "\n",
    "**Existiert Artikel im Englischen Wiki?**\n",
    "- **JA:** versuche Organisationen mit englischem Modell zu erkennen\n",
    "- **NEIN:** existiert Artikel im Deutschen Wiki? \n",
    "    - **JA:** versuche Organisationen mit deutschem Modell zu erkennen\n",
    "    - **NEIN:** aus Toleranz als Firma oder nicht Firma zählen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird diese Methode kurz getestet, um Sie später in der Sammlung von Firmennamen anwenden zu können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Page - Summary: Aldi (stylised as ALDI) is the common company brand name of \n",
      "ALDI ORG\n",
      "two CARDINAL\n",
      "German NORP\n",
      "10,000 CARDINAL\n",
      "20 CARDINAL\n",
      "Karl PERSON\n",
      "Theo Albrecht PERSON\n",
      "1946 DATE\n",
      "Essen GPE\n",
      "two CARDINAL\n",
      "1960 DATE\n",
      "Aldi Nord PERSON\n",
      "Essen GPE\n",
      "Aldi Süd PERSON\n",
      "Aldi NORP\n",
      "Germany GPE\n",
      "Mülheim GPE\n",
      "1962 DATE\n",
      "Aldi PERSON\n",
      "Albrecht Diskont ORG\n",
      "Germany GPE\n",
      "Aldi Nord PERSON\n",
      "Aldi Süd PERSON\n",
      "1966 DATE\n",
      "Aldi Nord PERSON\n",
      "Aldi Einkauf GmbH & Co. ORG\n",
      "Aldi Sud PERSON\n",
      "German NORP\n",
      "Aldi Nord's PERSON\n",
      "35 CARDINAL\n",
      "about 2,500 CARDINAL\n",
      "Germany GPE\n",
      "Aldi Süd's PERSON\n",
      "32 CARDINAL\n",
      "1,900 CARDINAL\n",
      "Germany GPE\n",
      "Aldi Nord PERSON\n",
      "Denmark GPE\n",
      "France GPE\n",
      "Benelux GPE\n",
      "Portugal GPE\n",
      "Spain GPE\n",
      "Poland GPE\n",
      "Aldi Süd PERSON\n",
      "Ireland GPE\n",
      "the United Kingdom GPE\n",
      "Hungary GPE\n",
      "Switzerland GPE\n",
      "Australia GPE\n",
      "China GPE\n",
      "Italy GPE\n",
      "Austria GPE\n",
      "Slovenia GPE\n",
      "Aldi Nord PERSON\n",
      "Aldi Süd PERSON\n",
      "Aldi PERSON\n",
      "the United States GPE\n",
      "1,600 CARDINAL\n",
      "2017 DATE\n",
      "U.S. GPE\n",
      "Aldi NORP\n",
      "Germany GPE\n",
      "2020 DATE\n",
      "Aldi Nord PERSON\n",
      "Aldi Süd PERSON\n",
      "2018 DATE\n",
      "two CARDINAL\n",
      "2022 DATE\n"
     ]
    }
   ],
   "source": [
    "# Tests:\n",
    "\n",
    "page = en_wiki.page(\"Aldi\") # Benennung der gesuchten Wikipediaseite als \"Aldi\"\n",
    "\n",
    "print(page.exists()) # Überprüfung ob Seite existiert\n",
    "\n",
    "print(\"Page - Summary: %s\" % page.summary[0:60]) # Ausgabe eines Zusammenfassungteils\n",
    "\n",
    "doc = en_nlp(page.summary) # Laden der Seitenzusammenfassung in englisches NLP-Modell\n",
    "\n",
    "for ent in doc.ents: # Iterieren über erkannte Entitäten\n",
    "    print(ent.text, ent.label_) # Ausgabe der Entitätenart und des Titels\n",
    "\n",
    "# Nun kann sich ein Bild gemacht werden, ob die Zusammenfassung Entitäten liefert, die darauf hindeuten, dass es sich bei \"Aldi\" um eine Firma handelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können mithilfe der obigen Funktionen und mithilfe von im Folgenden erklärten regex-Mustern mit erhöhter Zuverlässigkeit Firmen aus dem Dump extrahiert werden.\n",
    "Diese werden anschließend der Textdatei `title-categories-map.txt` abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding company names...:   4%|▍         | 6104/155401 [01:07<27:19, 91.04it/s]  \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*') # Regex-Kriterium für Erkennung von \"Kategorie\" Feld, welches sich unten auf jeder Wikipedia-Seite befindet.\n",
    "list_of_words = ['hersteller', 'unternehmen', 'Unternehmen', 'Hersteller'] # Sollten keywords dieser Liste in dem \"Kategorien\"-Feld enthalten sein, liefert dies Hinweise darauf, dass es sich bei dem Thema des Artikels um ein Unternehmen handelt.\n",
    "uncertainty_markers = ['unternehmens', 'Unternehmens'] # Allerding gibt es auch Ausschlusskriterien, die zunächst nur auf mögliche Unsicherheiten hinweisen: könnte es sich in der Kategorie z.B. um einen Untenehmensberater handeln und nicht direkt ein Unternehmen?\n",
    "words_re = re.compile(\"|\".join(list_of_words)) # Erstellung des Regex-Musters mit Liste\n",
    "uncertainty_re = re.compile(\"|\".join(uncertainty_markers)) # Erstellung des Regex-Musters mit Liste\n",
    "\n",
    "with open(\"../data/title-categories-map.txt\", mode=\"w\") as outfile: # Eine Textdatei wird im Schreibemodus erstellt oder geöffnet und bildet Ablage für erkannte Firmen.\n",
    "    for i, page in enumerate(tqdm(dump, total=dump_len, desc=\"Finding company names...\")): # Iteration über Seiten im Dump\n",
    "        try: # Try-Catch-Block, damit einzelne Fehler nicht zu Abbruch des Schreibprozesses führen - gerade bei großem Dump sehr wichtig\n",
    "            line = \"\" # Erstellung der Lines, welche später in Textdatei abgebildet werden.\n",
    "            line += page.title + \",\" # Der Titel der Wikipediaseite bildet das erste Attribut der Zeile und wird per Komma separiert\n",
    "            for revision in page: # Iteration über revisions\n",
    "                for match in pattern.finditer(revision.text): # Anwendung des ersten Regex-Patterns, welches auf Kategorienfeld hindeutet oder nicht.\n",
    "                    line += match.group(1) + \",\" \n",
    "            line = line.strip(\",\")\n",
    "            \n",
    "            #lines.append(line)\n",
    "            if not words_re.search(str(line)): # Anwendung des zweiten Regex-Patterns, um grob zu erkennen, ob es sich bei der seite um die eines Unternehmens handeln könnte, oder nicht.\n",
    "                continue # Page überspringen, wenn es sich definitiv nicht um ein Unternehmen handelt\n",
    "            if uncertainty_re.search(str(line)): # Das nächste Regex-Pattern weist auf Unsicherheiten hin, da oft \"Unternehmens\" + \"andere Wortteil\", darauf hindeuten, dass Artikel selbst nur etwas, mit der Firma im Zusammenhang setehendes, thematisiert.\n",
    "                reduced_uncertaincy = reduceUncertainty(page.title)\n",
    "                if not reduced_uncertaincy: # Um hier mehr Gewissheit zu erlangen werden in den vereinzelten Fällen die eigenen Wikipediaartikel der Entitätenuntersucht. True wird zurückgegeben, wenn es sich um Firma handeln könnte.\n",
    "                    # True bei reduceUncertainty() weist auf Firma hin, bei False wird die for-Loop fortgesetzt wird über continue.\n",
    "                    continue\n",
    "            \n",
    "            # Wenn es sich ganz sicher um ein Unternehmen handel -> In die Datei schreiben\n",
    "            outfile.write(line+\"\\n\")\n",
    "\n",
    "            # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "            if ONLY_SAMPLE:\n",
    "                break\n",
    "        except:\n",
    "            # Hier kann falls notwendig ein Fehlerhinweis ausgegeben werden.\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Extraktion der Infobox\n",
    "\n",
    "Der Aufbau einer Infobox eines Unternehmens in dessen Wikipedia Artikel ist streng definiert. Die Infobox beginnt mit zwei geschweiften Klammern und dem Typ des Eintrags, in diesem Fall {{{Infobox Unternehmen. Sie enthält maximal 14 Einträge: Name, Logo, Unternehmensform, ISIN, Gründungsdatum, Auflösungsdatum, Sitz, Leitung, Mitarbeiterzahl, Umsatz, Stand, Branche, Website. Erforderlich sind hiervon Unternehmensform sowie Sitz. \n",
    "\n",
    "Die Infobox wird mit folgendem Regex Pattern definiert: `((?<={{Infobox Unternehmen.).*?(?<=Homepage).*?(}})).`\n",
    "Es wird der Inhalt nach {{Infobox Unternehmen bis einschließlich dem letzten Eintrag Homepage.*?}} ausgelesen. \n",
    "\n",
    "> Verbesserungsvorschlag: Einträge einzeln auslesen, sodass nur die Parameter ausgegeben werden, die einen Wert besitzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting infoboxes...:   0%|          | 122/155401 [00:00<02:22, 1086.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(131, 1411), match=' Name              = Apple Inc.| Logo            >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#getting the infobox of a company\n",
    "\n",
    "infobox = re.compile('((?<={{Infobox Unternehmen.).*?(?<=Homepage).*?(}}))') #regex pattern gets text between keyword 'Infobox Unternehmen' and 'Homepage' \n",
    "\n",
    "stopme = False\n",
    "with open(\"../data/infobox.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump, total=dump_len, desc=\"Extracting infoboxes...\")):\n",
    "        # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "        if stopme:\n",
    "            break\n",
    "        line = ''\n",
    "        for revision in page: \n",
    "            text = re.sub(r'\\n', '', revision.text) #removing linebreaks\n",
    "            for match in infobox.finditer(text):\n",
    "                print(match)\n",
    "                line += match.group(1) + \"\\n\" \n",
    "                line = \" \".join(line.split()) #if there are multiple whitespaces, all except for one get deleted \n",
    "                outfile.write(line+\"\\n\")\n",
    "                # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "                if ONLY_SAMPLE:\n",
    "                    stopme = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "num_lines = sum(1 for line in open('../data/infobox.txt'))\n",
    "print(num_lines)\n",
    "#Output:697"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unternehmen ohne Infobox\n",
    "\n",
    "Nun soll auch für Unternehmen ohne Infobox deren Namen sowie dessen Webseite ausgegeben werden. \n",
    "\n",
    "Hierfür wird im ersten Schritt geprüft, ob der Wikipedia Artikel des Unternehmens das Wort Infobox enthält. Dies wird mit folgendem Regex Pattern geprüft: `(?!.*?Infobox)^.*$` \n",
    "Enthält der Artikel das Wort nicht, so erkennt das Regex Pattern den gesamten Artikel. \n",
    "Wird der Artikel als Unternehmensartikel klassifiziert und enthält er das Wort Infobox nicht, wird der Artikelname (=Unternehmensname) ausgegeben. \n",
    "\n",
    "Um nun auch die Webseite des Unternehmens ausgeben zu lassen, wird folgendes Regex Pattern angewandt: \n",
    "`.*((?<=Weblinks...).*?(?=...Einzelnachweise)).*`\n",
    "Dies gibt alle in der Sektion Weblinks eingetragnenen Webseiten aus, worunter oftmals auch die offizielle Unternehmenswebseite fällt. \n",
    "\n",
    "> Verbesserungsvorschlag: Es werden viele Artikel ausgelesen, die die Kategorie Unternehmsart enthalten, aber keine Unternehmen sind. Diese sind oftmals allgemeine Informationsseiten und enthalten daher auch keine Infobox, weshalb sie vermehrt ausgelesen werden. Darüber hinaus wird für die Unternehmenswebseite die ganze Sektion der Weblinks ausgelesen, die oftmals auch andere weiterführende Links enthält. Eine Verbesserungsmöglichkeit bestände darin, den Namen des Unternehmens in den Links zu suchen und nur die Links, die den Namen enthalten, auszugegeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting companies without infoboxes...:   1%|          | 1486/155401 [00:41<1:11:17, 35.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#Companies without infobox\n",
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*')\n",
    "list_of_words = ['hersteller', 'unternehmen', 'Unternehmen', 'Hersteller']\n",
    "words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "no_infobox = re.compile('(?!.*?Infobox)^.*$') #gets the whole article if it doesn´t contain 'Infobox'\n",
    "pattern_website = re.compile('.*((?<=Weblinks...).*?(?=...Einzelnachweise)).*') \n",
    "\n",
    "stopme = False\n",
    "with open(\"../data/company_name&website.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump, total=dump_len, desc=\"Extracting companies without infoboxes...\")):\n",
    "        # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "        if stopme:\n",
    "            break\n",
    "        line = \"\"\n",
    "        line += page.title\n",
    "        website = \"\"\n",
    "        for revision in page:\n",
    "            revision = re.sub(r'\\n', '', revision.text) #removing the line breaks \n",
    "            for match in pattern.finditer(revision):\n",
    "                line += match.group(1)  \n",
    "        \n",
    "        if words_re.search(str(line)):\n",
    "            for match in no_infobox.finditer(revision): #getting companies without an infobox \n",
    "                for match in pattern_website.finditer(revision): #getting the weblinks which contain the company url \n",
    "                    outfile.write(page.title+\"\\n\") #printing the page title = company name \n",
    "                    website += match.group(1)\n",
    "                    outfile.write(website+\"\\n\"+\"\\n\")\n",
    "                    # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "                    if ONLY_SAMPLE:\n",
    "                        stopme = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "companies_without_infobox = sum((1/3) for line in open('../data/company_name&website.txt'))\n",
    "print(companies_without_infobox)\n",
    "#Output: 136 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um nun eine Kurzbeschreibung des Unternehmens zu erhalten, wird der erste Satz des Artikels erfasst. Dieser enthält bei Unternehmen den Unternehmensnamen eingefasst in ''' '''. Aus diesem Grund wird folgendes Regex Pattern angewandt, um den ersten Satz auszulesen: \n",
    "((?<=\\''' ).*?(?<=\\.)+) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting short description...:   0%|          | 38/155401 [00:00<08:08, 317.86it/s]\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*')\n",
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*')\n",
    "list_of_words = ['hersteller', 'unternehmen', 'Unternehmen', 'Hersteller']\n",
    "words_re = re.compile(\"|\".join(list_of_words))\n",
    "\n",
    "pattern_firstsentence = re.compile('((?<=\\''' ).*?(?<=\\.))')\n",
    "#Erster Satz fängt meistens mit dem Firmennamen in ''' ''' an. Dann wird der erste Punkt gesucht.\n",
    "\n",
    "with open(\"../data/firstsentence.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump, total=dump_len, desc=\"Extracting short description...\")):\n",
    "        line = \"\"\n",
    "        line += page.title\n",
    "        firstsentence = \"\"\n",
    "        for revision in page:\n",
    "            for match in pattern.finditer(revision.text):\n",
    "                line += match.group(1)  \n",
    "        \n",
    "        if words_re.search(str(line)):\n",
    "            for match in pattern_firstsentence.finditer(revision.text):\n",
    "                firstsentence += match.group(1)\n",
    "                firstsentence.replace(r'\\*', '')\n",
    "            outfile.write(page.title+\"\\n\")\n",
    "            outfile.write(firstsentence+\"\\n\"+\"\\n\")\n",
    "            # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "            if ONLY_SAMPLE:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "firstsentences = sum((1/3) for line in open('../data/firstsentence.txt'))\n",
    "print(firstsentences)\n",
    "#Output: 1570"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('forschungsseminar-nlp-PHREsQ0p-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e81bfc76e4edf5eaac366b2d174cdc806bcd54ed715e8fa404f22fdaba1607b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
