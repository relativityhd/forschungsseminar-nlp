{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Download der Wikipedia Daten\n",
    "\n",
    "Das herunterladen der Dump-Datei kann über einen der folgenden Befehle ausgeführt werden:\n",
    "\n",
    "```sh\n",
    "# Please run the commands from the root directory of this repository. To check run\n",
    "$ pwd\n",
    "/path/to/repositories/forschungsseminar-nlp\n",
    "# Download subset for testing/ development\n",
    "$ sh ./scripts/download-dump.sh\n",
    "# Dowload everything for application\n",
    "$ sh ./scripts/download-large-dump.sh\n",
    "```\n",
    "\n",
    ">Diese Befehle **müssen** im Root-Directory dieses Repositories ausgeführt werden!\n",
    "\n",
    "Zur Auswahl stehen hier\n",
    "\n",
    "1. eine Dumpdatei mit einer kleineren Größe von ca. 5GB (im Prozess entstehend) für ein effizientes Arbeiten und\n",
    "2. die allumfassende Dumpdatei mit einer Größe von ca. 28GB (im Prozess entstehend)\n",
    "\n",
    "Über die Variable `ONLY_SAMPLE` kann bestimmt werden ob nur ein Subset der Daten beispielhaft verarbeitet werden oder der komplette große Dump verarbeitet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLY_SAMPLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraktion des Dumps:\n",
    "\n",
    "Die Bibliothek \"MediaWiki XML Processing\" oder auch \"mwxml\" ermöglicht ein vereinfachtes Streamen XML-Dumps über eine Abstrahierung.\n",
    "Über den mwxml.Dump werden die Wikipediaartikel auf eine iterative Weise über mwxml.Page‘s zugänglich gemacht.\n",
    "Im Folgenden werden über die angegebene Datei (entweder die kleine oder große XML-Datei) ein mwxml-Dump erstellt, welcher eben diese Attribute bereitstellt.\n",
    "Um die Funktionalität und Fehlerfreiheit zu prüfen werden im Folgenden zudem die Info der Seite und der Name der untersuchten Datenbank ausgegeben. Zu erwarten wären hier Hinweise auf die deutsche Wikipediafassung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Wikipedia', 'dewiki', 5235974)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importieren der Bibliotheken und Module\n",
    "import mwxml # Für Untersuchung der XMLs\n",
    "import re # Für spätere regex-Operationen\n",
    "from tqdm import tqdm # Für die Integration von Fortschrittsbannern, Ladebalken etc.\n",
    "\n",
    "def loadDump():\n",
    "    if ONLY_SAMPLE:\n",
    "        dump = mwxml.Dump.from_file(open(\"../data/dewiki-20220520-pages-articles-multistream1.xml\"))\n",
    "    else:\n",
    "        #dewiki-20220620-pages-articles-multistream.xml als Alternative, falls großer Dump genutzt werden soll.\n",
    "        dump = mwxml.Dump.from_file(open(\"../data/dewiki-20220620-pages-articles-multistream.xml\"))\n",
    "    return dump\n",
    "\n",
    "dump = loadDump()\n",
    "dump_len = 155401 if ONLY_SAMPLE else 5235974 # Da uns durch ausprobieren die Längen beider Dateien bekannt sind, kann hier Zeit gespart werden indem die Längen hardcoded werden. Muss für neue Dateien entsprechend entfert werden\n",
    "#dump_len = sum(1 for _ in dump) # Das verbraucht den 'items' generator des dumps, weshalb 'dump' neu geladen \n",
    "#dump = loadDump()\n",
    "dump.site_info.name, dump.site_info.dbname, dump_len # Ausgabe der Informationen zum Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Sammeln der Firmennamen aus Dump\n",
    "\n",
    "Im Folgenden wird vorläufig eine später verwendete Methodik und Funktion zur weiteren Verbesserung der Firmenklassifikation implementiert.\n",
    "Für eine spätetere Ausräumung von Unsicherheiten in deutscher Wiki werden sowohl deutsches NER-Modell auf deutscher Wiki und englisches Modell auf englischer Wiki genutzt. \n",
    "Auch mit deutschem Modell auf rein deutscher Wikipedia möglich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.23.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: jinja2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (60.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.3.0/de_core_news_sm-3.3.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from de-core-news-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.23.1)\n",
      "Requirement already satisfied: jinja2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.7.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (60.6.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/tobias/.cache/pypoetry/virtualenvs/forschungsseminar-nlp-PHREsQ0p-py3.8/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm # Herunterladen des englischen Spacy-Sprachmodells per Bash-Skript - erfahrungsgemäß bessere Zuverlässigkeit.\n",
    "!python -m spacy download de_core_news_sm # Herunterladen des deutschsprachigen Spacy-Sprachmodells per Bash-Skript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die nötigen Modelle heruntergeladen wurden, kann nun die Funktion zur Verbesserung der Genauigkeit implementiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firmenerkennung über NER\n",
    "import pandas as pd\n",
    "import spacy # spacy ist eine Bibliothek für fortgeschrittene Natürliche Sprachverarbeitung\n",
    "import wikipediaapi # Vorteil: sowohl für 'de' als auch 'en' nutzbar: die wikipediapi ist eine Implementierung der Wikipedia-Api als eine \"Wikipedia Python API\". So können sehr effizient Zusammenfassungen und Seiten zu gesuchten Titeln angefordert werden.\n",
    "\n",
    "en_nlp = spacy.load(\"en_core_web_sm\") # Laden des englischen Spacy-Modells für NER\n",
    "de_nlp = spacy.load(\"de_core_news_sm\") # Laden des deutschen Spacy-Modells für NER\n",
    "en_wiki = wikipediaapi.Wikipedia('en') # Laden der englischen wikipediaapi-Version\n",
    "de_wiki = wikipediaapi.Wikipedia('de') # Laden des deutschen wikipediaapi-Version\n",
    "\n",
    "# Regex-Pattern\n",
    "pattern = re.compile('.*\\[\\[Kategorie:(.*)\\]\\].*') # Regex-Kriterium für Erkennung von \"Kategorie\" Feld, welches sich unten auf jeder Wikipedia-Seite befindet.\n",
    "list_of_words = ['hersteller', 'unternehmen', 'Unternehmen', 'Hersteller'] # Sollten keywords dieser Liste in dem \"Kategorien\"-Feld enthalten sein, liefert dies Hinweise darauf, dass es sich bei dem Thema des Artikels um ein Unternehmen handelt.\n",
    "uncertainty_markers = ['unternehmens', 'Unternehmens'] # Allerding gibt es auch Ausschlusskriterien, die zunächst nur auf mögliche Unsicherheiten hinweisen: könnte es sich in der Kategorie z.B. um einen Untenehmensberater handeln und nicht direkt ein Unternehmen?\n",
    "words_re = re.compile(\"|\".join(list_of_words)) # Erstellung des Regex-Musters mit Liste\n",
    "uncertainty_re = re.compile(\"|\".join(uncertainty_markers)) # Erstellung des Regex-Musters mit Liste\n",
    "\n",
    "\n",
    "def reduceUncertainty(page_title):\n",
    "    \"\"\"Diese Funktion soll abgerufen werden können, falls bei einer erkannten Entität nicht klar ist, ob es sich um eine Firma handelt. Hierdurch sollen Unsicherheiten reduziert werden.\n",
    "    \"\"\"\n",
    "    en_page = en_wiki.page(page_title) # Das Funktionsattribut page_title enthält den gesuchten Firmenname/Wikipediatitel und wird hier in der englischen API gesucht. \n",
    "    de_page = de_wiki.page(page_title) # Das Funktionsattribut page_title enthält den gesuchten Firmenname/Wikipediatitel und wird hier über die deutsche API gesucht.\n",
    "\n",
    "    if not en_page.exists() and not de_page.exists(): # Englische und Deutsche Wikipedia-Seite existiert nicht\n",
    "        return False # gibt False(\"company not exists\") zurück, wenn die Seite nicht gefunden werden kann. So werden nur Seiten entfernt über die wir Zusatzwissen erlangen konnten und am wenigsten drastisch eingegriffen. Je nach Business Case kann dies anders Implementiert werden.\n",
    "\n",
    "    # Deutsche Seite existiert aber Englische Seite existiert nicht - in diesem Fall muss idealerweise ein deutsches NER-Modell verwendet werden\n",
    "    doc =  en_nlp(en_page.summary) if en_page.exists() else de_nlp(de_page.summary) # Die Zusammenfassung sollte in den meisten Fällen für die Urteilsfällung ausreichen\n",
    "    org_list = [] # Diese Liste wird leer erklärt, um später alle erwähnten Organisationen in dem Betrefflichen Firmenartikel zu erkennen.\n",
    "    for ent in doc.ents: # Alle erkannten Entitäten werden hier gesammelt\n",
    "        if (ent.label_ == 'ORG') and (page_title in ent.text): # ... Und anschließend gefiltert: Zunächst sollen nur erkannte Organisationen behalten werden, welche zudem in ihrem Eigennamen den Titel der untersuchten Seite als String enthalten. Zukünftig könnten hier erweiterte Regex-Operationen verwendet werden.\n",
    "            org_list.append(ent) # Nur diese, den Kriterien unterliegende Entitäten, werden der Liste von erkannten Organisationen hinzugefügt.\n",
    "    return len(org_list) > 0\n",
    "\n",
    "def getPageText(page: mwxml.iteration.page.Page) -> list:\n",
    "    \"\"\"\n",
    "    Diese Funktion extrahiert die Texte aller Revisions einer Wikipedia Page des Dumps. Dies ist notwendig, da mwxml ausschließlich mit Iteratoren arbeitet, welche lediglich einmal benutzt werden können. So werden die Texte extrahiert und sie sind wiederholbar iterabel.\n",
    "    \"\"\"\n",
    "    texts = [revision.text for revision in page]\n",
    "    return texts\n",
    "\n",
    "def getCategoriesFromPage(texts: list) -> list:\n",
    "    \"\"\"\n",
    "    Diese Funktion extrahiert Wikipedias Kategorien aus den Texten \n",
    "    \"\"\"\n",
    "    # Extrahieren von Kategorien, dazu wird zuerst über alle Texte der Revisions der Wikipedia Page iteriert und darauf über alle matches des ersten Regex-Patterns, welches auf Kategorienfeld hindeutet oder nicht.\n",
    "    categories = [match.group(1) for text in texts for match in pattern.finditer(text)]\n",
    "    return categories\n",
    "\n",
    "def isCompany(categories: list, page_title: str) -> bool:\n",
    "    \"\"\"\n",
    "    Diese Funktion zeigt ob die Kategorien auf ein Unternehmen hindeuten oder nicht\n",
    "    \"\"\"\n",
    "    \n",
    "    line = \",\".join(categories) # Um diese Kategorien weiter zu verarbeiten werden sie als ein großer String zusammen gefasst\n",
    "\n",
    "    if not words_re.search(str(line)): # Anwendung des zweiten Regex-Patterns, um grob zu erkennen, ob es sich bei der seite um die eines Unternehmens handeln könnte, oder nicht.\n",
    "        return False # Page überspringen, wenn es sich definitiv nicht um ein Unternehmen handelt\n",
    "    if uncertainty_re.search(str(line)): # Das nächste Regex-Pattern weist auf Unsicherheiten hin, da oft \"Unternehmens\" + \"andere Wortteil\", darauf hindeuten, dass Artikel selbst nur etwas, mit der Firma im Zusammenhang setehendes, thematisiert.\n",
    "        return reduceUncertainty(page_title) # Um hier mehr Gewissheit zu erlangen werden in den vereinzelten Fällen die eigenen Wikipediaartikel der Entitätenuntersucht. True wird zurückgegeben, wenn es sich um Firma handeln könnte.\n",
    "    # Wenn vorher nicht ausgefiltert wurde handelt es sich um ein Unternehmen\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Obiges Skript durchläuft also den folgenden logischen Prozess bzw. Algorithums:\n",
    "\n",
    "**Existiert Artikel im Englischen Wiki?**\n",
    "- **JA:** versuche Organisationen mit englischem Modell zu erkennen\n",
    "- **NEIN:** existiert Artikel im Deutschen Wiki? \n",
    "    - **JA:** versuche Organisationen mit deutschem Modell zu erkennen\n",
    "    - **NEIN:** aus Toleranz als Firma oder nicht Firma zählen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird diese Methode kurz getestet, um Sie später in der Sammlung von Firmennamen anwenden zu können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Page - Summary: Aldi (stylised as ALDI) is the common company brand name of \n",
      "ALDI ORG\n",
      "two CARDINAL\n",
      "German NORP\n",
      "10,000 CARDINAL\n",
      "20 CARDINAL\n",
      "Karl PERSON\n",
      "Theo Albrecht PERSON\n",
      "1946 DATE\n",
      "Essen GPE\n",
      "two CARDINAL\n",
      "1960 DATE\n",
      "Aldi Nord PERSON\n",
      "Essen GPE\n",
      "Aldi Süd PERSON\n",
      "Mülheim GPE\n",
      "1962 DATE\n",
      "Aldi PERSON\n",
      "Albrecht Diskont ORG\n",
      "Germany GPE\n",
      "Aldi Nord PERSON\n",
      "Aldi Süd PERSON\n",
      "1966 DATE\n",
      "Aldi Nord PERSON\n",
      "Aldi Einkauf GmbH & Co. ORG\n",
      "Aldi Sud PERSON\n",
      "German NORP\n",
      "Aldi Nord's PERSON\n",
      "35 CARDINAL\n",
      "about 2,500 CARDINAL\n",
      "Germany GPE\n",
      "Aldi Süd's PERSON\n",
      "32 CARDINAL\n",
      "1,900 CARDINAL\n",
      "Germany GPE\n",
      "Aldi Nord PERSON\n",
      "Denmark GPE\n",
      "France GPE\n",
      "Benelux GPE\n",
      "Portugal GPE\n",
      "Spain GPE\n",
      "Poland GPE\n",
      "Aldi Süd PERSON\n",
      "Ireland GPE\n",
      "the United Kingdom GPE\n",
      "Hungary GPE\n",
      "Switzerland GPE\n",
      "Australia GPE\n",
      "China GPE\n",
      "Italy GPE\n",
      "Austria GPE\n",
      "Slovenia GPE\n",
      "Aldi Nord PERSON\n",
      "Aldi Süd PERSON\n",
      "Aldi PERSON\n",
      "the United States GPE\n",
      "1,600 CARDINAL\n",
      "2017 DATE\n",
      "U.S. GPE\n",
      "Aldi NORP\n",
      "Germany GPE\n",
      "2020 DATE\n",
      "Aldi Nord PERSON\n",
      "Aldi Süd PERSON\n",
      "2018 DATE\n",
      "two CARDINAL\n",
      "2022 DATE\n"
     ]
    }
   ],
   "source": [
    "# Tests:\n",
    "\n",
    "page = en_wiki.page(\"Aldi\") # Benennung der gesuchten Wikipediaseite als \"Aldi\"\n",
    "\n",
    "print(page.exists()) # Überprüfung ob Seite existiert\n",
    "\n",
    "print(\"Page - Summary: %s\" % page.summary[0:60]) # Ausgabe eines Zusammenfassungteils\n",
    "\n",
    "doc = en_nlp(page.summary) # Laden der Seitenzusammenfassung in englisches NLP-Modell\n",
    "\n",
    "for ent in doc.ents: # Iterieren über erkannte Entitäten\n",
    "    print(ent.text, ent.label_) # Ausgabe der Entitätenart und des Titels\n",
    "\n",
    "# Nun kann sich ein Bild gemacht werden, ob die Zusammenfassung Entitäten liefert, die darauf hindeuten, dass es sich bei \"Aldi\" um eine Firma handelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können mithilfe der obigen Funktionen und mithilfe von im Folgenden erklärten regex-Mustern mit erhöhter Zuverlässigkeit Firmen aus dem Dump extrahiert werden.\n",
    "Diese werden anschließend der Textdatei `title-categories-map.txt` abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding companies...: 100%|██████████| 5235974/5235974 [2:11:34<00:00, 663.25it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "      <th>dump_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple| , Hardwarehersteller (Vereinigte Staate...</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aldi</td>\n",
       "      <td>Aldi| , Abkürzung, Einzelhandelsunternehmen (D...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E-Plus</td>\n",
       "      <td>Ehemaliger Mobilfunkanbieter, Telekommunikatio...</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First National</td>\n",
       "      <td>Ehemalige Filmgesellschaft (Vereinigte Staaten...</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GfK (Unternehmen)</td>\n",
       "      <td>Marktforschungsunternehmen, Dienstleistungsunt...</td>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71225</th>\n",
       "      <td>Aspen Holdings Logo.svg</td>\n",
       "      <td>Datei:Logo (Unternehmen aus Südafrika)</td>\n",
       "      <td>5235225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71226</th>\n",
       "      <td>Schürfeld</td>\n",
       "      <td>Papierhersteller, Unternehmen (Hamburg)</td>\n",
       "      <td>5235253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71227</th>\n",
       "      <td>GUSCO</td>\n",
       "      <td>Handelsunternehmen (Hamburg)</td>\n",
       "      <td>5235255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71228</th>\n",
       "      <td>Pulverfabrik Skodawerke-Wetzler</td>\n",
       "      <td>Österreichische Wirtschaftsgeschichte, Ehemali...</td>\n",
       "      <td>5235474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71229</th>\n",
       "      <td>SM Investments</td>\n",
       "      <td>Unternehmen (Philippinen), Pasay, Gegründet 1960</td>\n",
       "      <td>5235522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71230 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0                                Apple   \n",
       "1                                 Aldi   \n",
       "2                               E-Plus   \n",
       "3                       First National   \n",
       "4                    GfK (Unternehmen)   \n",
       "...                                ...   \n",
       "71225          Aspen Holdings Logo.svg   \n",
       "71226                        Schürfeld   \n",
       "71227                            GUSCO   \n",
       "71228  Pulverfabrik Skodawerke-Wetzler   \n",
       "71229                   SM Investments   \n",
       "\n",
       "                                              categories  dump_index  \n",
       "0      Apple| , Hardwarehersteller (Vereinigte Staate...         121  \n",
       "1      Aldi| , Abkürzung, Einzelhandelsunternehmen (D...         180  \n",
       "2      Ehemaliger Mobilfunkanbieter, Telekommunikatio...         960  \n",
       "3      Ehemalige Filmgesellschaft (Vereinigte Staaten...        1004  \n",
       "4      Marktforschungsunternehmen, Dienstleistungsunt...        1243  \n",
       "...                                                  ...         ...  \n",
       "71225             Datei:Logo (Unternehmen aus Südafrika)     5235225  \n",
       "71226            Papierhersteller, Unternehmen (Hamburg)     5235253  \n",
       "71227                       Handelsunternehmen (Hamburg)     5235255  \n",
       "71228  Österreichische Wirtschaftsgeschichte, Ehemali...     5235474  \n",
       "71229   Unternehmen (Philippinen), Pasay, Gegründet 1960     5235522  \n",
       "\n",
       "[71230 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies = [] # Liste welche später genutzt wird um einen DataFrame zu erstellen. Speichert Titel und Kategorien in einem Dict je Unternehmen\n",
    "for i, page in enumerate(tqdm(dump, total=dump_len, desc=\"Finding companies...\")): # Iteration über Seiten im Dump\n",
    "    try: # Try-Catch-Block, damit einzelne Fehler nicht zu Abbruch des Schreibprozesses führen - gerade bei großem Dump sehr wichtig\n",
    "        texts = getPageText(page)\n",
    "        categories = getCategoriesFromPage(texts)\n",
    "        is_company = isCompany(categories, page.title)\n",
    "\n",
    "        # Wenn Page kein Unternehmen ist soll sie übersprungen werden und nicht zur Liste der Unternehmen hinzugefügt werden\n",
    "        if not is_company:\n",
    "            continue\n",
    "\n",
    "        # Einfügen in die Liste der Unternehmen\n",
    "        companies.append({\n",
    "            \"title\": page.title,\n",
    "            \"categories\": \", \".join(categories),\n",
    "            \"dump_index\": i\n",
    "        })\n",
    "\n",
    "        # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "        if ONLY_SAMPLE:\n",
    "            break\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except SystemExit as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Fehler bei {page.title} ({i})\")\n",
    "        tqdm.write(repr(e))\n",
    "        continue\n",
    "\n",
    "companies = pd.DataFrame(companies)\n",
    "companies.to_feather(\"../data/company-categories.feather\")\n",
    "companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_dump_idx = set(companies[\"dump_index\"]) # Ist später nützlich, um die recht aufwendige Funktionen 'getCategoriesFromPage' und 'isCompany' nicht nochmals auszuführen, sondern beim erneutem iterieren über den dump lediglich `if not i in company_dump_idx: continue` aufgerufen werden kann. Speicherung als set, da sets deutlich schneller als pandas Series und normale Listen sind wenn es um solche aufrufe geht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del companies # Speicherplatz freigeben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Extraktion der Infobox\n",
    "\n",
    "Der Aufbau einer Infobox eines Unternehmens in dessen Wikipedia Artikel ist streng definiert. Die Infobox beginnt mit zwei geschweiften Klammern und dem Typ des Eintrags, in diesem Fall `{{{Infobox Unternehmen`. Sie enthält maximal 14 Einträge: Name, Logo, Unternehmensform, ISIN, Gründungsdatum, Auflösungsdatum, Sitz, Leitung, Mitarbeiterzahl, Umsatz, Stand, Branche, Website. Erforderlich sind hiervon Unternehmensform sowie Sitz. \n",
    "\n",
    "Die Infobox wird mit folgendem Regex Pattern definiert: `((?<={{Infobox Unternehmen.).*?(?<=Homepage).*?(}})).`\n",
    "Es wird der Inhalt nach {{Infobox Unternehmen bis einschließlich dem letzten Eintrag Homepage.*?}} ausgelesen. \n",
    "\n",
    "> Verbesserungsvorschlag: Einträge einzeln auslesen, sodass nur die Parameter ausgegeben werden, die einen Wert besitzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting infoboxes...: 100%|██████████| 5235974/5235974 [12:54<00:00, 6763.85it/s] \n"
     ]
    }
   ],
   "source": [
    "dump = loadDump() # Da im Aufgabenteil B der dump Iterator aufgebraucht wurde, muss dieser neu geladen werden\n",
    "infobox = re.compile('((?<={{Infobox Unternehmen.).*?(?<=Homepage).*?(}}))') # Regex Pattern erkennt den Text der Infobox zwischen den Keywörtern Unternehmen und Homepage\n",
    "\n",
    "def isCompanyByIdx(i, page, texts):\n",
    "    \"\"\"\n",
    "    Funktion welche deutlich schneller entscheidet ob eine Page übersprungen werden muss. Nutz den in Aufgabe B erstellen Dump-Index. Wenn ONLY_SAMPLE == True ist wurde dieser Index nicht vollständig erstellt, weshalb auf die normale Methode zurückgegriffen wird.\n",
    "    \"\"\"\n",
    "    if ONLY_SAMPLE:\n",
    "        categories = getCategoriesFromPage(texts)\n",
    "        is_company = isCompany(categories, page.title)\n",
    "        return is_company\n",
    "\n",
    "    return i in company_dump_idx\n",
    "\n",
    "\n",
    "with open(\"../data/infobox.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump, total=dump_len, desc=\"Extracting infoboxes...\")):\n",
    "        try: # Try-Catch-Block, damit einzelne Fehler nicht zu Abbruch des Schreibprozesses führen - gerade bei großem Dump sehr wichtig\n",
    "            texts = getPageText(page)\n",
    "\n",
    "            # Wenn Page kein Unternehmen ist soll sie übersprungen werden\n",
    "            if not isCompanyByIdx(i, page, texts):\n",
    "                continue\n",
    "\n",
    "            line = ''\n",
    "            for text in texts:\n",
    "                text = re.sub(r'\\n', '', text) # Linebreaks entfernen\n",
    "                for match in infobox.finditer(text):\n",
    "                    line += match.group(1) + \"\\n\" \n",
    "                    line = \" \".join(line.split()) # Löschen mehrerer Whitespaces\n",
    "                    outfile.write(line+\"\\n\")\n",
    "\n",
    "            # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "            if ONLY_SAMPLE:\n",
    "                break\n",
    "\n",
    "        except KeyboardInterrupt as e:\n",
    "            raise e\n",
    "        except SystemExit as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Fehler bei {page.title} ({i})\")\n",
    "            tqdm.write(repr(e))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24771"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines = sum(1 for line in open('../data/infobox.txt'))\n",
    "num_lines\n",
    "#Output: 24771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unternehmen ohne Infobox\n",
    "\n",
    "Nun soll auch für Unternehmen ohne Infobox deren Namen sowie dessen Webseite ausgegeben werden. \n",
    "\n",
    "Hierfür wird im ersten Schritt geprüft, ob der Wikipedia Artikel des Unternehmens das Wort Infobox enthält. Dies wird mit folgendem Regex Pattern geprüft: `(?!.*?Infobox)^.*$` \n",
    "Enthält der Artikel das Wort nicht, so erkennt das Regex Pattern den gesamten Artikel. \n",
    "Wird der Artikel als Unternehmensartikel klassifiziert und enthält er das Wort Infobox nicht, wird der Artikelname (=Unternehmensname) ausgegeben. \n",
    "\n",
    "Um nun auch die Webseite des Unternehmens ausgeben zu lassen, wird folgendes Regex Pattern angewandt: \n",
    "`.*((?<=Weblinks...).*?(?=...Einzelnachweise)).*`\n",
    "Dies gibt alle in der Sektion Weblinks eingetragnenen Webseiten aus, worunter oftmals auch die offizielle Unternehmenswebseite fällt. \n",
    "\n",
    "> Verbesserungsvorschlag: Es werden viele Artikel ausgelesen, die die Kategorie Unternehmsart enthalten, aber keine Unternehmen sind. Diese sind oftmals allgemeine Informationsseiten und enthalten daher auch keine Infobox, weshalb sie vermehrt ausgelesen werden. Darüber hinaus wird für die Unternehmenswebseite die ganze Sektion der Weblinks ausgelesen, die oftmals auch andere weiterführende Links enthält. Eine Verbesserungsmöglichkeit bestände darin, den Namen des Unternehmens in den Links zu suchen und nur die Links, die den Namen enthalten, auszugegeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting companies without infoboxes...: 100%|██████████| 5235974/5235974 [1:33:44<00:00, 930.87it/s]  \n"
     ]
    }
   ],
   "source": [
    "#Companies without infobox\n",
    "dump = loadDump() # Da im Aufgabenteil B der dump Iterator aufgebraucht wurde, muss dieser neu geladen werden\n",
    "\n",
    "pattern_website = re.compile('.*((?<=Weblinks...).*?(?=...Einzelnachweise)).*') \n",
    "\n",
    "with open(\"../data/company_name&website.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump, total=dump_len, desc=\"Extracting companies without infoboxes...\")):\n",
    "        try: # Try-Catch-Block, damit einzelne Fehler nicht zu Abbruch des Schreibprozesses führen - gerade bei großem Dump sehr wichtig\n",
    "            texts = getPageText(page)\n",
    "\n",
    "            # Wenn Page kein Unternehmen ist soll sie übersprungen werden\n",
    "            if not isCompanyByIdx(i, page, texts):\n",
    "                #pass\n",
    "                continue\n",
    "            \n",
    "\n",
    "            found = False\n",
    "            for text in texts:\n",
    "                text = re.sub(r'\\n', '', text) # Linebreaks entfernen\n",
    "\n",
    "                # Unternehmen mit Infoboxen überspringen\n",
    "                if infobox.search(text):\n",
    "                    continue\n",
    "\n",
    "                for match in pattern_website.finditer(text): # Suche nach Weblinks\n",
    "                    outfile.write(page.title+\"\\n\") # Ausgabe des Page Titels\n",
    "                    website = match.group(1)\n",
    "                    outfile.write(website+\"\\n\"+\"\\n\")\n",
    "                    found = True\n",
    "\n",
    "            # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "            if ONLY_SAMPLE and found:\n",
    "                break\n",
    "        \n",
    "        except KeyboardInterrupt as e:\n",
    "            raise e\n",
    "        except SystemExit as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Fehler bei {page.title} ({i})\")\n",
    "            tqdm.write(repr(e))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15837.000000010925"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_without_infobox = sum((1//3) for line in open('../data/company_name&website.txt'))\n",
    "companies_without_infobox\n",
    "#Output: 15837"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um nun eine Kurzbeschreibung des Unternehmens zu erhalten, wird der erste Satz des Artikels erfasst. Dieser enthält bei Unternehmen den Unternehmensnamen eingefasst in ''' '''. Aus diesem Grund wird folgendes Regex Pattern angewandt, um den ersten Satz auszulesen: \n",
    "((?<=\\''' ).*?(?<=\\.)+) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting short description...: 100%|██████████| 5235974/5235974 [11:51<00:00, 7357.68it/s] \n"
     ]
    }
   ],
   "source": [
    "dump = loadDump() # Da im Aufgabenteil B der dump Iterator aufgebraucht wurde, muss dieser neu geladen werden\n",
    "pattern_firstsentence = re.compile('((?<=\\''' ).*?(?<=\\.))')\n",
    "#Erster Satz fängt meistens mit dem Firmennamen in ''' ''' an. Dann wird der erste Punkt gesucht.\n",
    "\n",
    "with open(\"../data/firstsentence.txt\", mode=\"w\") as outfile:\n",
    "    for i, page in enumerate(tqdm(dump, total=dump_len, desc=\"Extracting short description...\")):\n",
    "        try: # Try-Catch-Block, damit einzelne Fehler nicht zu Abbruch des Schreibprozesses führen - gerade bei großem Dump sehr wichtig\n",
    "            texts = getPageText(page)\n",
    "\n",
    "            # Wenn Page kein Unternehmen ist soll sie übersprungen werden\n",
    "            if not isCompanyByIdx(i, page, texts):\n",
    "                continue\n",
    "\n",
    "            firstsentence = \"\"\n",
    "            for text in texts:\n",
    "                text = re.sub(r'\\n', '', text) # Linebreaks entfernen\n",
    "\n",
    "                firstsentence = \"\"\n",
    "                for match in pattern_firstsentence.finditer(text):\n",
    "                    firstsentence += match.group(1)\n",
    "                    firstsentence.replace(r'\\*', '')\n",
    "                outfile.write(page.title+\"\\n\")\n",
    "                outfile.write(firstsentence+\"\\n\"+\"\\n\")\n",
    "\n",
    "            # Abbrechen wenn nur ein einziges Sample geladen werden soll\n",
    "            if ONLY_SAMPLE:\n",
    "                break\n",
    "            \n",
    "        except KeyboardInterrupt as e:\n",
    "            raise e\n",
    "        except SystemExit as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Fehler bei {page.title} ({i})\")\n",
    "            tqdm.write(repr(e))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71230.00000010787"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstsentences = sum((1//3) for line in open('../data/firstsentence.txt'))\n",
    "firstsentences\n",
    "#Output: 71230"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('forschungsseminar-nlp-PHREsQ0p-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e81bfc76e4edf5eaac366b2d174cdc806bcd54ed715e8fa404f22fdaba1607b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
